{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Extract** all data from CSV files.\n",
    "2. **Transform** data into inputs/targets.\n",
    "3. **Split** inputs/targets into train/test sets.\n",
    "4. **Sample** inputs/targets for the testing sets.\n",
    "5. **Load** outputs inputs/targets to parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS\n",
    "\n",
    "## schemas\n",
    "```python\n",
    "\n",
    "class MetadataSchema(Schema):\n",
    "    \"\"\"Schema for metadata in outputs.\"\"\"\n",
    "\n",
    "    timestamp: papd.Series[padt.String] = pa.Field()\n",
    "    model_version: papd.Series[padt.String] = pa.Field()\n",
    "\n",
    "\n",
    "class InputsSchema(Schema):\n",
    "    \"\"\"Schema for validating large string inputs.\"\"\"\n",
    "\n",
    "    input: papd.Series[padt.String] = pa.Field()\n",
    "\n",
    "\n",
    "class OutputsSchema(Schema):\n",
    "    \"\"\"Schema for structured JSON outputs.\"\"\"\n",
    "\n",
    "    response: papd.Series[padt.String] = pa.Field()\n",
    "    metadata: papd.Series[padt.Object] = pa.Field()\n",
    "\n",
    "\n",
    "class TargetsSchema(Schema):\n",
    "    \"\"\"Schema for the project target.\"\"\"\n",
    "\n",
    "    input: papd.Series[padt.String] = pa.Field()\n",
    "    response: papd.Series[padt.String] = pa.Field()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_llm = pd.read_json(\"hf://datasets/Vezora/Tested-143k-Python-Alpaca/143k-Tested-Python-Alpaca-Vezora.json\")\n",
    "df_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df_llm.drop(columns=['input', 'output'])\n",
    "df_input = df_input.rename(columns={'instruction': 'input'})\n",
    "\n",
    "df_target = df_llm.drop(columns=['instruction'])\n",
    "df_target = df_target.rename(columns={'output': 'response'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE = False # time-sensitive\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.15\n",
    "SAMPLE_RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS_TRAIN_FILE = \"../data/inputs_train.parquet\"\n",
    "INPUTS_TEST_FILE = \"../data/inputs_test.parquet\"\n",
    "TARGETS_TRAIN_FILE = \"../data/targets_train.parquet\"\n",
    "TARGETS_TEST_FILE = \"../data/targets_test.parquet\"\n",
    "INPUTS_SAMPLE_FILE = \"../tests/data/inputs_sample.parquet\"\n",
    "TARGETS_SAMPLE_FILE = \"../tests/data/targets_sample.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(\n",
    "    df_input, df_target, test_size=TEST_SIZE, shuffle=SHUFFLE\n",
    ")\n",
    "inputs_train.shape, inputs_test.shape, targets_train.shape, targets_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train_sample = inputs_train.sample(frac=SAMPLE_RATIO, random_state=SAMPLE_RANDOM_STATE)\n",
    "targets_train_sample = targets_train.sample(frac=SAMPLE_RATIO, random_state=SAMPLE_RANDOM_STATE)\n",
    "inputs_train_sample.shape, targets_train_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train.to_parquet(INPUTS_TRAIN_FILE)\n",
    "inputs_test.to_parquet(INPUTS_TEST_FILE)\n",
    "targets_train.to_parquet(TARGETS_TRAIN_FILE)\n",
    "targets_test.to_parquet(TARGETS_TEST_FILE)\n",
    "inputs_train_sample.to_parquet(INPUTS_SAMPLE_FILE)\n",
    "targets_train_sample.to_parquet(TARGETS_SAMPLE_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
