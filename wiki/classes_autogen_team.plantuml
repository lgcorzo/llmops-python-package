@startuml classes_autogen_team
set namespaceSeparator none
class "Adapter" as autogen_team.io.registries.CustomSaver.Adapter {
  model
  model_config : dict
  load_context(context: PythonModelContext) -> None
  predict(context: PythonModelContext, model_input: schemas.Inputs) -> schemas.Outputs
}
class "Adapter" as autogen_team.io.registries.Loader.Adapter {
  {abstract}load_context(model_config: Dict[str, Any]) -> None
  {abstract}predict(inputs: schemas.Inputs) -> schemas.Outputs
}
class "Adapter" as autogen_team.io.registries.CustomLoader.Adapter {
  model
  load_context(model_config: Dict[str, Any]) -> None
  predict(inputs: schemas.Inputs) -> schemas.Outputs
}
class "AlertsService" as autogen_team.io.services.AlertsService {
  app_name : str
  enable : bool
  timeout : int | None
  notify(title: str, message: str) -> None
  {abstract}start() -> None
}
class "AutogenConversationMetric" as autogen_team.core.metrics.AutogenConversationMetric {
  KIND : T.Literal['AutogenConversationMetric']
  check_error_messages : bool
  check_termination : bool
  score(targets: pd.DataFrame, outputs: pd.DataFrame) -> float
}
class "AutogenMetric" as autogen_team.core.metrics.AutogenMetric {
  KIND : T.Literal['AutogenMetric']
  metric_type : T.Literal['exact_match', 'similarity', 'length_ratio']
  similarity_threshold : Optional[float]
  score(targets: pd.DataFrame, outputs: pd.DataFrame) -> float
}
class "BaseModel" as pydantic.main.BaseModel {
  model_computed_fields : ClassVar[dict[str, ComputedFieldInfo]]
  model_config : ClassVar[ConfigDict]
  model_extra
  model_fields : ClassVar[dict[str, FieldInfo]]
  model_fields_set
  construct(_fields_set: set[str] | None) -> Self
  copy() -> Self
  dict() -> Dict[str, Any]
  from_orm(obj: Any) -> Self
  json() -> str
  model_construct(_fields_set: set[str] | None) -> Self
  model_copy() -> Self
  model_dump() -> dict[str, Any]
  model_dump_json() -> str
  model_json_schema(by_alias: bool, ref_template: str, schema_generator: type[GenerateJsonSchema], mode: JsonSchemaMode) -> dict[str, Any]
  model_parametrized_name(params: tuple[type[Any], ...]) -> str
  {abstract}model_post_init(__context: Any) -> None
  model_rebuild() -> bool | None
  model_validate(obj: Any) -> Self
  model_validate_json(json_data: str | bytes | bytearray) -> Self
  model_validate_strings(obj: Any) -> Self
  parse_file(path: str | Path) -> Self
  parse_obj(obj: Any) -> Self
  parse_raw(b: str | bytes) -> Self
  schema(by_alias: bool, ref_template: str) -> Dict[str, Any]
  schema_json() -> str
  update_forward_refs() -> None
  validate(value: Any) -> Self
}
class "BaselineAutogenModel" as autogen_team.core.models.BaselineAutogenModel {
  KIND : T.Literal['BaselineAutogenModel']
  max_tokens : Optional[int]
  _model_client : Optional[Any]
  model_config_data : Optional[Dict[str, Any]]
  model_config_path : Optional[str]
  temperature : Optional[float]
  explain_model() -> schemas.FeatureImportances
  explain_samples(inputs: schemas.Inputs) -> schemas.SHAPValues
  fit(inputs: schemas.Inputs, targets: schemas.Targets) -> 'BaselineAutogenModel'
  get_internal_model() -> OpenAIChatClient
  load_context(model_config: Dict[str, Any]) -> None
  load_context_path(model_config_path: Optional[str]) -> None
  predict(inputs: schemas.Inputs) -> schemas.Outputs
}
class "Config" as autogen_team.core.schemas.Schema.Config {
  coerce : bool
  strict : bool
}
class "Config" as autogen_team.core.schemas.SHAPValuesSchema.Config {
  strict : bool
}
class "Config" as autogen_team.io.osvariables.Env.Config {
  case_sensitive : bool
  env_file : str
  env_file_encoding : str
}
class "CustomLoader" as autogen_team.io.registries.CustomLoader {
  KIND : T.Literal['CustomLoader']
  load(uri: str) -> 'CustomLoader.Adapter'
}
class "CustomSaver" as autogen_team.io.registries.CustomSaver {
  KIND : T.Literal['CustomSaver']
  save(model: models.Model, signature: signers.Signature, input_example: schemas.Inputs) -> Info
}
class "Env" as autogen_team.io.osvariables.Env {
  mlflow_experiment_name : str
  mlflow_registered_model_name : str
  mlflow_registry_uri : str
  mlflow_tracking_uri : str
}
class "EvaluationsJob" as autogen_team.jobs.evaluations.EvaluationsJob {
  KIND : T.Literal['EvaluationsJob']
  alias_or_version : T.Union[str, int]
  evaluators : List[str]
  inputs
  metrics : List[metrics_.AutogenMetric]
  model_type : str
  run_config
  targets
  thresholds : Dict[str, metrics_.Threshold]
  run() -> base.Locals
}
class "ExplanationsJob" as autogen_team.jobs.explanations.ExplanationsJob {
  KIND : T.Literal['ExplanationsJob']
  alias_or_version : str | int
  inputs_samples
  loader
  models_explanations
  samples_explanations
  run() -> base.Locals
}
class "FeatureImportancesSchema" as autogen_team.core.schemas.FeatureImportancesSchema {
  feature : papd.Series[padt.String]
  importance : papd.Series[padt.Float32]
}
class "GridCVSearcher" as autogen_team.utils.searchers.GridCVSearcher {
  KIND : T.Literal['GridCVSearcher']
  error_score : str | float
  n_jobs : int | None
  refit : bool
  return_train_score : bool
  verbose : int
  search(model: models.Model, metric: metrics.Metric, inputs: schemas.Inputs, targets: schemas.Targets, cv: CrossValidation) -> Results
}
class "InferSigner" as autogen_team.utils.signers.InferSigner {
  KIND : T.Literal['InferSigner']
  sign(inputs: schemas.Inputs, outputs: schemas.Outputs) -> Signature
}
class "InferenceJob" as autogen_team.jobs.inference.InferenceJob {
  KIND : T.Literal['InferenceJob']
  alias_or_version : str | int
  inputs
  loader
  outputs
  run() -> base.Locals
}
class "InputsSchema" as autogen_team.core.schemas.InputsSchema {
  input : papd.Series[padt.String]
}
class "Job" as autogen_team.jobs.base.Job {
  KIND : str
  alerts_service
  logger_service
  mlflow_service
  {abstract}run() -> Locals
}
class "Loader" as autogen_team.io.registries.Loader {
  KIND : str
  {abstract}load(uri: str) -> 'Loader.Adapter'
}
class "LoggerService" as autogen_team.io.services.LoggerService {
  backtrace : bool
  catch : bool
  colorize : bool
  diagnose : bool
  format : str
  level : str
  serialize : bool
  sink : str
  logger() -> loguru.Logger
  start() -> None
}
class "MainSettings" as autogen_team.settings.MainSettings {
  job
}
class "MetadataSchema" as autogen_team.core.schemas.MetadataSchema {
  model_version : papd.Series[padt.String]
  timestamp : papd.Series[padt.String]
}
class "Metric" as autogen_team.core.metrics.Metric {
  KIND : str
  greater_is_better : bool
  name : str
  {abstract}score(targets: pd.DataFrame, outputs: pd.DataFrame) -> float
  scorer(model: models.Model, inputs: schemas.Inputs, targets: pd.DataFrame) -> float
  to_mlflow() -> MlflowMetric
}
class "MlflowRegister" as autogen_team.io.registries.MlflowRegister {
  KIND : T.Literal['MlflowRegister']
  register(name: str, model_uri: str) -> Version
}
class "MlflowService" as autogen_team.io.services.MlflowService {
  autolog_disable : bool
  autolog_disable_for_unsupported_versions : bool
  autolog_exclusive : bool
  autolog_log_datasets : bool
  autolog_log_input_examples : bool
  autolog_log_model_signatures : bool
  autolog_log_models : bool
  autolog_silent : bool
  env : ClassVar[Env]
  experiment_name : str
  registry_name : str
  registry_uri : str
  tracking_uri : str
  client() -> mt.MlflowClient
  run_context(run_config: RunConfig) -> T.Generator[mlflow.ActiveRun, None, None]
  start() -> None
}
class "Model" as autogen_team.core.models.Model {
  KIND : str
  {abstract}explain_model() -> schemas.FeatureImportances
  {abstract}explain_samples(inputs: schemas.Inputs) -> schemas.SHAPValues
  {abstract}fit(inputs: schemas.Inputs, targets: schemas.Targets) -> T.Self
  {abstract}get_internal_model() -> T.Any
  get_params(deep: bool) -> Params
  {abstract}load_context(model_config: Dict[str, Any]) -> None
  {abstract}predict(inputs: schemas.Inputs) -> schemas.Outputs
  set_params() -> T.Self
}
class "OutputsSchema" as autogen_team.core.schemas.OutputsSchema {
  metadata : papd.Series[padt.Object]
  response : papd.Series[padt.String]
}
class "ParquetReader" as autogen_team.io.datasets.ParquetReader {
  KIND : T.Literal['ParquetReader']
  path : str
  lineage(name: str, data: pd.DataFrame, targets: str | None, predictions: str | None) -> Lineage
  read() -> pd.DataFrame
}
class "ParquetWriter" as autogen_team.io.datasets.ParquetWriter {
  KIND : T.Literal['ParquetWriter']
  path : str
  write(data: pd.DataFrame) -> None
}
class "PromotionJob" as autogen_team.jobs.promotion.PromotionJob {
  KIND : T.Literal['PromotionJob']
  alias : str
  version : int | None
  run() -> base.Locals
}
class "Reader" as autogen_team.io.datasets.Reader {
  KIND : str
  limit : int | None
  {abstract}lineage(name: str, data: pd.DataFrame, targets: str | None, predictions: str | None) -> Lineage
  {abstract}read() -> pd.DataFrame
}
class "Register" as autogen_team.io.registries.Register {
  KIND : str
  tags : dict[str, T.Any]
  {abstract}register(name: str, model_uri: str) -> Version
}
class "RunConfig" as autogen_team.io.services.MlflowService.RunConfig {
  description : str | None
  log_system_metrics : bool | None
  name : str
  tags : dict[str, T.Any] | None
}
class "SHAPValuesSchema" as autogen_team.core.schemas.SHAPValuesSchema {
  explanation : papd.Series[padt.String]
  sample : papd.Series[padt.String]
  shap_value : papd.Series[padt.Float32]
}
class "Saver" as autogen_team.io.registries.Saver {
  KIND : str
  config_file : str
  path : str
  {abstract}save(model: models.Model, signature: signers.Signature, input_example: schemas.Inputs) -> Info
}
class "Schema" as autogen_team.core.schemas.Schema {
  check(data: pd.DataFrame) -> papd.DataFrame[TSchema]
}
class "Searcher" as autogen_team.utils.searchers.Searcher {
  KIND : str
  param_grid : dict
  {abstract}search(model: models.Model, metric: metrics.Metric, inputs: schemas.Inputs, targets: schemas.Targets, cv: CrossValidation) -> Results
}
class "Service" as autogen_team.io.services.Service {
  {abstract}start() -> None
  {abstract}stop() -> None
}
class "Settings" as autogen_team.settings.Settings {
}
class "Signer" as autogen_team.utils.signers.Signer {
  KIND : str
  {abstract}sign(inputs: schemas.Inputs, outputs: schemas.Outputs) -> Signature
}
class "Singleton" as autogen_team.io.osvariables.Singleton {
}
class "Splitter" as autogen_team.utils.splitters.Splitter {
  KIND : str
  {abstract}get_n_splits(inputs: schemas.Inputs, targets: schemas.Targets, groups: Index | None) -> int
  {abstract}split(inputs: schemas.Inputs, targets: schemas.Targets, groups: Index | None) -> TrainTestSplits
}
class "TargetsSchema" as autogen_team.core.schemas.TargetsSchema {
  input_target : papd.Series[padt.String]
  response : papd.Series[padt.String]
}
class "Threshold" as autogen_team.core.metrics.Threshold {
  greater_is_better : bool
  threshold : int | float
  to_mlflow() -> MlflowThreshold
}
class "TimeSeriesSplitter" as autogen_team.utils.splitters.TimeSeriesSplitter {
  KIND : T.Literal['TimeSeriesSplitter']
  gap : int
  n_splits : int
  test_size : int | float
  get_n_splits(inputs: schemas.Inputs, targets: schemas.Targets, groups: Index | None) -> int
  split(inputs: schemas.Inputs, targets: schemas.Targets, groups: Index | None) -> TrainTestSplits
}
class "TrainTestSplitter" as autogen_team.utils.splitters.TrainTestSplitter {
  KIND : T.Literal['TrainTestSplitter']
  random_state : int
  shuffle : bool
  test_size : int | float
  get_n_splits(inputs: schemas.Inputs, targets: schemas.Targets, groups: Index | None) -> int
  split(inputs: schemas.Inputs, targets: schemas.Targets, groups: Index | None) -> TrainTestSplits
}
class "TrainingJob" as autogen_team.jobs.training.TrainingJob {
  KIND : T.Literal['TrainingJob']
  inputs
  metrics : list
  model
  registry
  run_config
  saver
  signer
  splitter
  targets
  run() -> base.Locals
}
class "TuningJob" as autogen_team.jobs.tuning.TuningJob {
  KIND : T.Literal['TuningJob']
  inputs
  metric
  model
  run_config
  searcher
  splitter
  targets
  run() -> base.Locals
}
class "Writer" as autogen_team.io.datasets.Writer {
  KIND : str
  {abstract}write(data: pd.DataFrame) -> None
}
autogen_team.core.metrics.AutogenConversationMetric --|> autogen_team.core.metrics.Metric
autogen_team.core.metrics.AutogenMetric --|> autogen_team.core.metrics.Metric
autogen_team.core.metrics.Metric --|> pydantic.main.BaseModel
autogen_team.core.metrics.Threshold --|> pydantic.main.BaseModel
autogen_team.core.models.BaselineAutogenModel --|> autogen_team.core.models.Model
autogen_team.core.models.Model --|> pydantic.main.BaseModel
autogen_team.core.schemas.FeatureImportancesSchema --|> autogen_team.core.schemas.Schema
autogen_team.core.schemas.InputsSchema --|> autogen_team.core.schemas.Schema
autogen_team.core.schemas.MetadataSchema --|> autogen_team.core.schemas.Schema
autogen_team.core.schemas.OutputsSchema --|> autogen_team.core.schemas.Schema
autogen_team.core.schemas.SHAPValuesSchema --|> autogen_team.core.schemas.Schema
autogen_team.core.schemas.TargetsSchema --|> autogen_team.core.schemas.Schema
autogen_team.io.datasets.ParquetReader --|> autogen_team.io.datasets.Reader
autogen_team.io.datasets.ParquetWriter --|> autogen_team.io.datasets.Writer
autogen_team.io.datasets.Reader --|> pydantic.main.BaseModel
autogen_team.io.datasets.Writer --|> pydantic.main.BaseModel
autogen_team.io.osvariables.Env --|> autogen_team.io.osvariables.Singleton
autogen_team.io.registries.CustomLoader --|> autogen_team.io.registries.Loader
autogen_team.io.registries.CustomLoader.Adapter --|> autogen_team.io.registries.Loader.Adapter
autogen_team.io.registries.CustomSaver --|> autogen_team.io.registries.Saver
autogen_team.io.registries.Loader --|> pydantic.main.BaseModel
autogen_team.io.registries.MlflowRegister --|> autogen_team.io.registries.Register
autogen_team.io.registries.Register --|> pydantic.main.BaseModel
autogen_team.io.registries.Saver --|> pydantic.main.BaseModel
autogen_team.io.services.AlertsService --|> autogen_team.io.services.Service
autogen_team.io.services.LoggerService --|> autogen_team.io.services.Service
autogen_team.io.services.MlflowService --|> autogen_team.io.services.Service
autogen_team.io.services.MlflowService.RunConfig --|> pydantic.main.BaseModel
autogen_team.io.services.Service --|> pydantic.main.BaseModel
autogen_team.jobs.base.Job --|> pydantic.main.BaseModel
autogen_team.jobs.evaluations.EvaluationsJob --|> autogen_team.jobs.base.Job
autogen_team.jobs.explanations.ExplanationsJob --|> autogen_team.jobs.base.Job
autogen_team.jobs.inference.InferenceJob --|> autogen_team.jobs.base.Job
autogen_team.jobs.promotion.PromotionJob --|> autogen_team.jobs.base.Job
autogen_team.jobs.training.TrainingJob --|> autogen_team.jobs.base.Job
autogen_team.jobs.tuning.TuningJob --|> autogen_team.jobs.base.Job
autogen_team.settings.MainSettings --|> autogen_team.settings.Settings
autogen_team.utils.searchers.GridCVSearcher --|> autogen_team.utils.searchers.Searcher
autogen_team.utils.searchers.Searcher --|> pydantic.main.BaseModel
autogen_team.utils.signers.InferSigner --|> autogen_team.utils.signers.Signer
autogen_team.utils.signers.Signer --|> pydantic.main.BaseModel
autogen_team.utils.splitters.Splitter --|> pydantic.main.BaseModel
autogen_team.utils.splitters.TimeSeriesSplitter --|> autogen_team.utils.splitters.Splitter
autogen_team.utils.splitters.TrainTestSplitter --|> autogen_team.utils.splitters.Splitter
autogen_team.core.models.BaselineAutogenModel --* autogen_team.jobs.training.TrainingJob : model
autogen_team.core.models.BaselineAutogenModel --* autogen_team.jobs.tuning.TuningJob : model
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.evaluations.EvaluationsJob : inputs
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.evaluations.EvaluationsJob : targets
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.explanations.ExplanationsJob : inputs_samples
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.inference.InferenceJob : inputs
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.training.TrainingJob : inputs
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.training.TrainingJob : targets
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.tuning.TuningJob : inputs
autogen_team.io.datasets.ParquetReader --* autogen_team.jobs.tuning.TuningJob : targets
autogen_team.io.datasets.ParquetWriter --* autogen_team.jobs.explanations.ExplanationsJob : models_explanations
autogen_team.io.datasets.ParquetWriter --* autogen_team.jobs.explanations.ExplanationsJob : samples_explanations
autogen_team.io.datasets.ParquetWriter --* autogen_team.jobs.inference.InferenceJob : outputs
autogen_team.io.registries.CustomLoader --* autogen_team.jobs.explanations.ExplanationsJob : loader
autogen_team.io.registries.CustomLoader --* autogen_team.jobs.inference.InferenceJob : loader
autogen_team.io.registries.CustomSaver --* autogen_team.jobs.training.TrainingJob : saver
autogen_team.io.registries.MlflowRegister --* autogen_team.jobs.training.TrainingJob : registry
autogen_team.io.services.AlertsService --* autogen_team.jobs.base.Job : alerts_service
autogen_team.io.services.LoggerService --* autogen_team.jobs.base.Job : logger_service
autogen_team.io.services.MlflowService --* autogen_team.jobs.base.Job : mlflow_service
autogen_team.io.services.MlflowService.RunConfig --* autogen_team.jobs.evaluations.EvaluationsJob : run_config
autogen_team.io.services.MlflowService.RunConfig --* autogen_team.jobs.training.TrainingJob : run_config
autogen_team.io.services.MlflowService.RunConfig --* autogen_team.jobs.tuning.TuningJob : run_config
autogen_team.utils.searchers.GridCVSearcher --* autogen_team.jobs.tuning.TuningJob : searcher
autogen_team.utils.signers.InferSigner --* autogen_team.jobs.training.TrainingJob : signer
autogen_team.core.models.Model --o autogen_team.io.registries.CustomSaver.Adapter : model
@enduml
